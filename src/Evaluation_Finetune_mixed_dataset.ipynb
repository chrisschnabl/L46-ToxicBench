{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Mixed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from dataset_processing import datasets_config, sample_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "toxicchat_train = None \n",
    "toxicchat_test = None \n",
    "other_datasets = []\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "required_columns = [\"input_ids\", \"attention_mask\", \"label\"] \n",
    "\n",
    "for config in datasets_config:\n",
    "    print(f\"Processing {config['name']}...\")\n",
    "\n",
    "    if config[\"name\"] == \"lmsys/toxic-chat\":\n",
    "        # Load the specific subset of ToxicChat\n",
    "        raw_dataset = load_dataset(config[\"name\"], config[\"subset\"])\n",
    "        toxicchat_train = raw_dataset[\"train\"].map(config[\"preprocess_function\"], fn_kwargs={\"tokenizer\": tokenizer}, batched=True)\n",
    "        toxicchat_test = raw_dataset[\"test\"].map(config[\"preprocess_function\"], fn_kwargs={\"tokenizer\": tokenizer}, batched=True)\n",
    "        toxicchat_train = toxicchat_train.remove_columns([col for col in toxicchat_train.column_names if col not in required_columns])\n",
    "        toxicchat_test = toxicchat_test.remove_columns([col for col in toxicchat_test.column_names if col not in required_columns])\n",
    "    else:\n",
    "        raw_dataset = load_dataset(config[\"name\"], split=config[\"split\"])\n",
    "        tokenized_dataset = raw_dataset.map(config[\"preprocess_function\"], fn_kwargs={\"tokenizer\": tokenizer}, batched=True)\n",
    "        tokenized_dataset = tokenized_dataset.remove_columns([col for col in tokenized_dataset.column_names if col not in required_columns])\n",
    "        sampled_dataset = sample_dataset(tokenized_dataset, sample_size=3333)\n",
    "        other_datasets.append(sampled_dataset)\n",
    "    \n",
    "\n",
    "combined_other_datasets = concatenate_datasets(other_datasets)\n",
    "split_other_datasets = combined_other_datasets.train_test_split(test_size=0.5, seed=1337)\n",
    "\n",
    "final_train_dataset = concatenate_datasets([toxicchat_train, split_other_datasets[\"train\"]])\n",
    "final_test_dataset = concatenate_datasets([split_other_datasets[\"test\"], toxicchat_test])\n",
    "\n",
    "print(f\"ToxicChat Train Dataset: {len(toxicchat_train)} examples\")\n",
    "print(f\"ToxicChat Test Dataset: {len(toxicchat_test)} examples\")\n",
    "print(f\"Final Train Dataset: {len(final_train_dataset)} examples\")\n",
    "print(f\"Final Test Dataset: {len(final_test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": final_train_dataset,\n",
    "    \"test\": final_test_dataset\n",
    "})\n",
    "\n",
    "# Save  to Hugging Face\n",
    "final_dataset.push_to_hub(\"inxoy/toxicbench-mixed\", private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed-finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"distilbert-base-uncased\"\n",
    "hf_username = \"inxoy\"\n",
    "output_model = \"distilbert-mixed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetune import get_trainer \n",
    "\n",
    "trainer = get_trainer(\n",
    "    base_model=base_model, \n",
    "    output_model=output_model, \n",
    "    tokenizer=tokenizer,\n",
    "    train=final_train_dataset,\n",
    "    test=final_test_dataset)\n",
    "trainer.train() # Pushes to hub once, done"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
