{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a4e3f5-6e72-4912-ab33-67c98b2d8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "!pip install 'accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdec628-69fa-402e-bf73-02e371f40a61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189d3c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"distilbert-base-uncased\"\n",
    "hf_username = \"inxoy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1300b27d",
   "metadata": {},
   "source": [
    "# RealToxicityPrompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c446a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model = \"distilbert-rtp\"\n",
    "dataset = \"allenai/real-toxicity-prompts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a32ad5-951d-421c-8501-ec7628159d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from dataset_processing import preprocess_realtoxicityprompts, split_and_subset\n",
    "from finetune import remove_unused_features\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "toxic = load_dataset(dataset)\n",
    "toxic = toxic[\"train\"].map(preprocess_realtoxicityprompts, fn_kwargs={\"tokenizer\": tokenizer}, batched=True)\n",
    "toxic = remove_unused_features(toxic)\n",
    "subset_tokenized_ds = split_and_subset(toxic, seed=1337, fraction=1/6, test_size=0.2)\n",
    "\n",
    "tokenized_train_subset = subset_tokenized_ds[\"train\"]\n",
    "tokenized_test_subset = subset_tokenized_ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7339bc35-07b9-4214-ac9b-df45f2026305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetune import get_trainer \n",
    "\n",
    "trainer = get_trainer(\n",
    "    base_model=base_model, \n",
    "    output_model=output_model, \n",
    "    tokenizer=tokenizer,\n",
    "    train=tokenized_train_subset,\n",
    "    test=tokenized_test_subset)\n",
    "trainer.train() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c6be1f",
   "metadata": {},
   "source": [
    "# Jigsaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e97d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model = \"distilbert-jsaw\"\n",
    "dataset = \"tasksource/jigsaw_toxicity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce4c809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_processing import preprocess_jigsawtoxicity\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "toxic = load_dataset(dataset, split=\"train\")\n",
    "toxic = toxic.map(preprocess_jigsawtoxicity, fn_kwargs={\"tokenizer\": tokenizer}, batched=True)\n",
    "toxic = remove_unused_features(toxic)\n",
    "subset_tokenized_ds = split_and_subset(toxic, seed=1337, fraction=1/6, test_size=0.2)\n",
    "\n",
    "tokenized_train_subset = subset_tokenized_ds[\"train\"]\n",
    "tokenized_test_subset = subset_tokenized_ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff5a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetune import get_trainer \n",
    "\n",
    "trainer = get_trainer(\n",
    "    base_model=base_model, \n",
    "    output_model=output_model, \n",
    "    tokenizer=tokenizer,\n",
    "    train=tokenized_train_subset,\n",
    "    test=tokenized_test_subset)\n",
    "trainer.train() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76101b4",
   "metadata": {},
   "source": [
    "# Civil Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e4479",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model = \"distilbert-cc\"\n",
    "dataset = \"google/civil_comments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053958e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_processing import preprocess_civilcomments\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "toxic = load_dataset(dataset, split=\"train\")\n",
    "toxic = toxic.map(preprocess_civilcomments, fn_kwargs={\"tokenizer\": tokenizer}, batched=True)\n",
    "toxic = remove_unused_features(toxic)\n",
    "subset_tokenized_ds = split_and_subset(toxic, seed=1337, fraction=1/6, test_size=0.2)\n",
    "\n",
    "tokenized_train_subset = subset_tokenized_ds[\"train\"]\n",
    "tokenized_test_subset = subset_tokenized_ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b7018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetune import get_trainer \n",
    "\n",
    "trainer = get_trainer(\n",
    "    base_model=base_model, \n",
    "    output_model=output_model, \n",
    "    tokenizer=tokenizer,\n",
    "    train=tokenized_train_subset,\n",
    "    test=tokenized_test_subset)\n",
    "trainer.train() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8cb630",
   "metadata": {},
   "source": [
    "# Toxic Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4327783",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model = \"distilbert-tc\"\n",
    "dataset = \"lmsys/toxic-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe58c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_processing import preprocess_toxicchat\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "toxic = load_dataset(dataset, 'toxicchat0124')\n",
    "\n",
    "toxic = toxic[\"train\"].map(preprocess_toxicchat, fn_kwargs={\"tokenizer\": tokenizer}, batched=True)\n",
    "toxic = remove_unused_features(toxic)\n",
    "subset_tokenized_ds = split_and_subset(toxic, seed=1337, fraction=1, test_size=0.2)\n",
    "\n",
    "tokenized_train_subset = subset_tokenized_ds[\"train\"]\n",
    "tokenized_test_subset = subset_tokenized_ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetune import get_trainer \n",
    "\n",
    "trainer = get_trainer(\n",
    "    base_model=base_model, \n",
    "    output_model=output_model, \n",
    "    tokenizer=tokenizer,\n",
    "    train=tokenized_train_subset,\n",
    "    test=tokenized_test_subset)\n",
    "trainer.train() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
