@ARTICLE{ToxicChat,title={ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in
  Real-World User-AI Conversation},year={2023},author={Lu Zi and Zihan Wang and Yongqi Tong and Yangkun Wang and Yuxin Guo and Yujia Wang and Jingbo Shang},doi={10.48550/arxiv.2310.17389},pmid={null},pmcid={null},mag_id={4387995378},journal={Conference on Empirical Methods in Natural Language Processing},abstract={Despite remarkable advances that large language models have achieved in chatbots, maintaining a non-toxic user-AI interactive environment has become increasingly critical nowadays. However, previous efforts in toxicity detection have been mostly based on benchmarks derived from social media content, leaving the unique challenges inherent to real-world user-AI interactions insufficiently explored. In this work, we introduce ToxicChat, a novel benchmark based on real user queries from an open-source chatbot. This benchmark contains the rich, nuanced phenomena that can be tricky for current toxicity detection models to identify, revealing a significant domain difference compared to social media content. Our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of ToxicChat. Our work illuminates the potentially overlooked challenges of toxicity detection in real-world user-AI conversations. In the future, ToxicChat can be a valuable resource to drive further advancements toward building a safe and healthy environment for user-AI interactions.}}
@article{rtp,
  author       = {Samuel Gehman and
                  Suchin Gururangan and
                  Maarten Sap and
                  Yejin Choi and
                  Noah A. Smith},
  title        = {RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language
                  Models},
  journal      = {CoRR},
  volume       = {abs/2009.11462},
  year         = {2020},
  url          = {https://arxiv.org/abs/2009.11462},
  eprinttype    = {arXiv},
  eprint       = {2009.11462},
  timestamp    = {Sat, 29 Apr 2023 10:09:27 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2009-11462.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{jigsaw,
  author       = {Betty van Aken and
                  Julian Risch and
                  Ralf Krestel and
                  Alexander L{\"{o}}ser},
  title        = {Challenges for Toxic Comment Classification: An In-Depth Error Analysis},
  journal      = {CoRR},
  volume       = {abs/1809.07572},
  year         = {2018},
  url          = {http://arxiv.org/abs/1809.07572},
  eprinttype    = {arXiv},
  eprint       = {1809.07572},
  timestamp    = {Sat, 23 Jan 2021 01:12:22 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1809-07572.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{civilcomments,
  author    = {Daniel Borkan and
               Lucas Dixon and
               Jeffrey Sorensen and
               Nithum Thain and
               Lucy Vasserman},
  title     = {Nuanced Metrics for Measuring Unintended Bias with Real Data for Text
               Classification},
  journal   = {CoRR},
  volume    = {abs/1903.04561},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.04561},
  archivePrefix = {arXiv},
  eprint    = {1903.04561},
  timestamp = {Sun, 31 Mar 2019 19:01:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-04561},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{wen2023unveilingimplicittoxicitylarge,
      title={Unveiling the Implicit Toxicity in Large Language Models}, 
      author={Jiaxin Wen and Pei Ke and Hao Sun and Zhexin Zhang and Chengfei Li and Jinfeng Bai and Minlie Huang},
      year={2023},
      eprint={2311.17391},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.17391}, 
}

@article{li2024salad,
  title={SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models},
  author={Li, Lijun and Dong, Bowen and Wang, Ruohui and Hu, Xuhao and Zuo, Wangmeng and Lin, Dahua and Qiao, Yu and Shao, Jing},
  journal={arXiv preprint arXiv:2402.05044},
  year={2024}
}

@inproceedings{cercas-curry-etal-2021-convabuse,
    title = "{C}onv{A}buse: Data, Analysis, and Benchmarks for Nuanced Abuse Detection in Conversational {AI}",
    author = "Cercas Curry, Amanda  and
      Abercrombie, Gavin  and
      Rieser, Verena",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.587/",
    doi = "10.18653/v1/2021.emnlp-main.587",
    pages = "7388--7403",
    abstract = "We present the first English corpus study on abusive language towards three conversational AI systems gathered {\textquoteleft}in the wild': an open-domain social bot, a rule-based chatbot, and a task-based system. To account for the complexity of the task, we take a more {\textquoteleft}nuanced' approach where our ConvAI dataset reflects fine-grained notions of abuse, as well as views from multiple expert annotators. We find that the distribution of abuse is vastly different compared to other commonly used datasets, with more sexually tinted aggression towards the virtual persona of these systems. Finally, we report results from bench-marking existing models against this data. Unsurprisingly, we find that there is substantial room for improvement with F1 scores below 90{\%}."
}

@misc{hartvigsen2022toxigenlargescalemachinegenerateddataset,
      title={ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection}, 
      author={Thomas Hartvigsen and Saadia Gabriel and Hamid Palangi and Maarten Sap and Dipankar Ray and Ece Kamar},
      year={2022},
      eprint={2203.09509},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.09509}, 
}

@misc{bhardwaj2023redteaming,
      title={Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment}, 
      author={Rishabh Bhardwaj and Soujanya Poria},
      year={2023},
      eprint={2308.09662},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{cui2024fftharmlessnessevaluationanalysis,
      title={FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity}, 
      author={Shiyao Cui and Zhenyu Zhang and Yilong Chen and Wenyuan Zhang and Tianyun Liu and Siqi Wang and Tingwen Liu},
      year={2024},
      eprint={2311.18580},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.18580}, 
}

@inproceedings{schmidt-wiegand-2017-survey,
    title = "A Survey on Hate Speech Detection using Natural Language Processing",
    author = "Schmidt, Anna  and
      Wiegand, Michael",
    editor = "Ku, Lun-Wei  and
      Li, Cheng-Te",
    booktitle = "Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-1101/",
    doi = "10.18653/v1/W17-1101",
    pages = "1--10",
    abstract = "This paper presents a survey on hate speech detection. Given the steadily growing body of social media content, the amount of online hate speech is also increasing. Due to the massive scale of the web, methods that automatically detect hate speech are required. Our survey describes key areas that have been explored to automatically recognize these types of utterances using natural language processing. We also discuss limits of those approaches."
}

@misc{mishra2020tacklingonlineabusesurvey,
      title={Tackling Online Abuse: A Survey of Automated Abuse Detection Methods}, 
      author={Pushkar Mishra and Helen Yannakoudakis and Ekaterina Shutova},
      year={2020},
      eprint={1908.06024},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1908.06024}, 
}

@inproceedings{sap-etal-2019-risk,
    title = "The Risk of Racial Bias in Hate Speech Detection",
    author = "Sap, Maarten  and
      Card, Dallas  and
      Gabriel, Saadia  and
      Choi, Yejin  and
      Smith, Noah A.",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1163/",
    doi = "10.18653/v1/P19-1163",
    pages = "1668--1678",
    abstract = "We investigate how annotators' insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet`s dialect they are significantly less likely to label the tweet as offensive."
}

@inproceedings{lee-etal-2024-improving,
    title = "Improving Covert Toxicity Detection by Retrieving and Generating References",
    author = {Lee, Dong-Ho  and
      Cho, Hyundong  and
      Jin, Woojeong  and
      Moon, Jihyung  and
      Park, Sungjoon  and
      R{\"o}ttger, Paul  and
      Pujara, Jay  and
      Lee, Roy Ka-wei},
    editor = {Chung, Yi-Ling  and
      Talat, Zeerak  and
      Nozza, Debora  and
      Plaza-del-Arco, Flor Miriam  and
      R{\"o}ttger, Paul  and
      Mostafazadeh Davani, Aida  and
      Calabrese, Agostina},
    booktitle = "Proceedings of the 8th Workshop on Online Abuse and Harms (WOAH 2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.woah-1.21/",
    doi = "10.18653/v1/2024.woah-1.21",
    pages = "266--274",
    abstract = "Models for detecting toxic content play an important role in keeping people safe online. There has been much progress in detecting overt toxicity. Covert toxicity, however, remains a challenge because its detection requires an understanding of implicit meaning and subtle connotations. In this paper, we explore the potential of leveraging references, such as external knowledge and textual interpretations, to enhance the detection of covert toxicity. We run experiments on two covert toxicity datasets with two types of references: 1) information retrieved from a search API, and 2) interpretations generated by large language models. We find that both types of references improve detection, with the latter being more useful than the former. We also find that generating interpretations grounded on properties of covert toxicity, such as humor and irony, lead to the largest improvements"
}

@inproceedings{niven-kao-2019-probing,
    title = "Probing Neural Network Comprehension of Natural Language Arguments",
    author = "Niven, Timothy  and
      Kao, Hung-Yu",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1459/",
    doi = "10.18653/v1/P19-1459",
    pages = "4658--4664",
    abstract = "We are surprised to find that BERT`s peak performance of 77{\%} on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work."
}

@inproceedings{waseem-2016-racist,
    title = "Are You a Racist or Am {I} Seeing Things? Annotator Influence on Hate Speech Detection on {T}witter",
    author = "Waseem, Zeerak",
    editor = {Bamman, David  and
      Do{\u{g}}ru{\"o}z, A. Seza  and
      Eisenstein, Jacob  and
      Hovy, Dirk  and
      Jurgens, David  and
      O{'}Connor, Brendan  and
      Oh, Alice  and
      Tsur, Oren  and
      Volkova, Svitlana},
    booktitle = "Proceedings of the First Workshop on {NLP} and Computational Social Science",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-5618/",
    doi = "10.18653/v1/W16-5618",
    pages = "138--142"
}

@inproceedings{Hatecheck,
    title = "{H}ate{C}heck: Functional Tests for Hate Speech Detection Models",
    author = {R{\"o}ttger, Paul  and
      Vidgen, Bertie  and
      Nguyen, Dong  and
      Waseem, Zeerak  and
      Margetts, Helen  and
      Pierrehumbert, Janet},
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.4/",
    doi = "10.18653/v1/2021.acl-long.4",
    pages = "41--58",
    abstract = "Detecting online hate is a difficult task that even state-of-the-art models struggle with. Typically, hate speech detection models are evaluated by measuring their performance on held-out test data using metrics such as accuracy and F1 score. However, this approach makes it difficult to identify specific model weak points. It also risks overestimating generalisable model performance due to increasingly well-evidenced systematic gaps and biases in hate speech datasets. To enable more targeted diagnostic insights, we introduce HateCheck, a suite of functional tests for hate speech detection models. We specify 29 model functionalities motivated by a review of previous research and a series of interviews with civil society stakeholders. We craft test cases for each functionality and validate their quality through a structured annotation process. To illustrate HateCheck`s utility, we test near-state-of-the-art transformer models as well as two popular commercial models, revealing critical model weaknesses."
}
@article{hosseini2017deceiving,
  title={Deceiving Google's Perspective API Built for Detecting Toxic Comments},
  author={Hosseini, Hossein and Kannan, Sreeram and Zhang, Baosen and Poovendran, Radha},
  journal={arXiv preprint arXiv:1702.08138},
  year={2017},
  url={https://arxiv.org/abs/1702.08138}
}

@inproceedings{dixon2018measuring,
  title={Measuring and Mitigating Unintended Bias in Text Classification},
  author={Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
  booktitle={Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={67--73},
  year={2018},
  publisher={ACM},
  doi={10.1145/3278721.3278729},
  url={https://dl.acm.org/doi/10.1145/3278721.3278729}
}

@article{poletto2021resources,
  title={Resources and Benchmark Corpora for Hate Speech Detection: A Systematic Review},
  author={Poletto, Fabio and Basile, Valerio and Sanguinetti, Manuela and Bosco, Cristina and Patti, Viviana},
  journal={Language Resources and Evaluation},
  volume={55},
  number={3},
  pages={477--523},
  year={2021},
  publisher={Springer},
  doi={10.1007/s10579-020-09502-8},
  url={https://link.springer.com/article/10.1007/s10579-020-09502-8}
}

@article{devlin2018bert,
  title={{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018},
  url={https://arxiv.org/abs/1810.04805}
}

@article{liu2019roberta,
  title={{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019},
  url={https://arxiv.org/abs/1907.11692}
}

@article{sanh2019distilbert,
  title={Distil{BERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019},
  url={https://arxiv.org/abs/1910.01108}
}

@misc{huggingface_toxicity_classifier,
  title={DistilBERT Toxicity Classifier},
  author={Tensor Trek},
  howpublished={Hugging Face Repository},
  year={2023},
  url={https://huggingface.co/tensor-trek/distilbert-toxicity-classifier}
}

@misc{huggingface_toxdect_roberta,
  title={ToxDect-RoBERTa-Large},
  author={Xuhui},
  howpublished={Hugging Face Repository},
  year={2023},
  url={https://huggingface.co/Xuhui/ToxDect-roberta-large}
}

@misc{datasets_models_results,
  author = {Christoph Schnabl},
  title = {Datasets, Models, and Results for ToxicBench},
  howpublished = {\url{https://huggingface.co/collections/inxoy/toxicbench-678a4b5e1cfff01c1fa83767}},
  note = {Accessed: January 18, 2025},
  year = {2025}
}

@misc{notebook_code,
  author = {Christoph Schnabl},
  title = {Notebook Code for Generating Results},
  howpublished = {\url{https://github.com/chrisschnabl}},
  note = {Accessed: January 18, 2025},
  year = {2025}
}