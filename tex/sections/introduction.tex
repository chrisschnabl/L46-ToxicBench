\section{Introduction}
Detecting toxic language is crucial for maintaining healthy online interactions~\cite{schmidt-wiegand-2017-survey, mishra2020tacklingonlineabusesurvey}. However, current models often struggle with generalization due to domain specificity~\cite{niven-kao-2019-probing, ToxicChat}, lack of standardized evaluation frameworks~\cite{poletto2021resources, sap-etal-2019-risk}, and robustness issues~\cite{Hatecheck, hosseini2017deceiving}. To address these challenges, we introduce ToxicBench, a comprehensive benchmark to evaluate toxicity detection models across datasets and contexts.\newline
Existing toxicity detection models frequently overfit to specific datasets that vary based on context and user interactions. For instance, models trained on social media data may underperform when applied to user-AI conversations, where the language and context differ significantly~\cite{ToxicChat}.\newline
The absence of standardized evaluation frameworks, including mitigation techniques, complicates the assessment of model performance. The lack of an agreed-upon definition of toxicity leads to inconsistencies in model evaluation and comparison~\cite{poletto2021resources}. Models are also vulnerable to adversarial attacks, such as spelling variations or negated phrases, raising concerns about their robustness in real-world applications~\cite{hosseini2017deceiving}.\newline
The annotation process for toxicity detection datasets also requires improvement. Documentation quality regarding annotators and their disagreements directly influences the reliability of these datasets~\cite{waseem-2016-racist, sap-etal-2019-risk}. Addressing annotator perspectives and resolving discrepancies can lead to more accurate and unbiased toxicity detection models.\newline
ToxicBench provides three contributions: (a) a systematic comparison of state-of-the-art classifiers in multiple domains to quantify generalization gaps in Section~\ref{sec:evaluation} 2.2, (b) an ablation study to isolate and improve cross-domain performance, and (c) a mixed-dataset fine-tuning approach that imrpoves model robustness in different contexts both in Section ~\ref{sec:evaluation} 2.3).