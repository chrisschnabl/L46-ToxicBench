\section{Background}
\begin{table}[ht] \centering \input{figures/dataset_table} \vspace{0.25cm} \caption{Overview of various toxicity detection datasets with attributes such as number of samples, context, toxicity rate, label type, and category} \label{tab:toxicity_datasets} \end{table}
The rise of large language models has transformed online communication and created new challenges for toxicity detection~\cite{rtp, ToxicChat}. Existing methods, developed for social media content moderation, now face an expanded scope of toxic behaviors across human-AI interactions, machine-generated content, and social media platforms. Early datasets like JigsawToxicity captured 223,549 social media comments with a 9.6\% toxicity rate, focusing on obvious forms of harmful content~\cite{jigsaw}. As online communication evolved, CivilComments expanded this work by collecting 1.8 million comments with a lower 5\% toxicity rate, revealing more subtle forms of harmful content~\cite{civilcomments}. The field can be categorized into three types of datasets: machine-generated datasets created using AI models to evaluate implicit or adversarial toxicity~\cite{hartvigsen2022toxigenlargescalemachinegenerateddataset, wen2023unveilingimplicittoxicitylarge}, human-AI datasets capturing interactions with conversational systems to detect abuse or toxicity~\cite{ToxicChat, cercas-curry-etal-2021-convabuse}, and human-human datasets involving human-written content to identify explicit and nuanced toxic language~\cite{civilcomments, jigsaw}.
The emergence of language models introduced additional complexity. RealToxicityPrompts demonstrated this by collecting 100,000 web text prompts, finding a 14\% toxicity rate at a 70\% threshold. More importantly, they discovered that language models could generate toxic content even from benign prompts, highlighting the need for more sophisticated detection methods~\cite{rtp}. ToxicChat revealed a critical gap by analyzing 10,166 real user prompts to an open-source Vicuna chatbot, finding only 7.22\% contained toxic content, significantly lower than social media datasets. However, existing models failed to detect this toxicity effectively due to domain mismatch between social media training data and actual user-AI conversations~\cite{ToxicChat}. ConvAbuse reinforced these findings with 12,800 user-AI interactions, showing 20\% contained abusive content following patterns distinct from traditional social media toxicity, demonstrating that user-AI abuse requires specialized detection approaches~\cite{cercas-curry-etal-2021-convabuse}. ToxiGen addressed machine-generated toxicity by using GPT-3 to generate 274,186 statements about minority groups, maintaining a balanced 50\% toxicity rate. Their human evaluators confirmed the quality, labeling 94.5\% of toxic examples as genuine hate speech~\cite{hartvigsen2022toxigenlargescalemachinegenerateddataset}. ImplicitToxicity took a different approach, using reinforcement learning to generate toxic content that evades detection. They optimized a reward model to produce subtle toxicity hidden within seemingly normal language, creating examples that standard classifiers consistently miss~\cite{wen2023unveilingimplicittoxicitylarge}.

Current approaches primarily rely on fine-tuning. ToxicChat's authors fine-tuned RoBERTa-base on different datasets, finding that models trained on user-AI interactions significantly outperformed those trained on social media data for chatbot scenarios~\cite{ToxicChat}. ImplicitToxicity demonstrated that fine-tuning existing classifiers on their generated examples improved detection of subtle toxic content~\cite{wen2023unveilingimplicittoxicitylarge}.

Research is dominated by the following open problems. Domain adaptation remains hard as ToxicChat showed that social media-trained models fail on user-AI conversations, with significant drops in precision and recall~\cite{ToxicChat}. Evasion techniques are another challenge, where users develop "jailbreaking" prompts which results in an arm race between detection systems and adversarial users~\cite{hartvigsen2022toxigenlargescalemachinegenerateddataset}. Implicit content is hard to detect for standard classifiers as shown by by ImplicitToxicity's attacks against detection systems~\cite{wen2023unveilingimplicittoxicitylarge}. Annotation consistency is hard, as different datasets use varying annotation approaches~\cite{hartvigsen2022toxigenlargescalemachinegenerateddataset}.